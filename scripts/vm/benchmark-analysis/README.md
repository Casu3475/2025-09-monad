# Benchmark Analysis Scripts

This directory contains two Python scripts that can be used to compare execution
benchmark results across different axes:
* Two versions of the same program against each other (e.g. before and after an
  optimisation is applied).
* Two implementations on the same benchmark (e.g. the Monad X86 compiler against
  evmone on the same program and input data).

The scripts are built using [UV](https://github.com/astral-sh/uv), which needs
to be installed before running the examples in this file.

##Â Generating Results

The Python scripts ingest JSON generated by Google's
[Benchmark](https://github.com/google/benchmark) library. To generate results
for two different versions of the same code, run:
```console
$ ./build/test/execution_benchmarks/execution-benchmarks  \
    --benchmark_repetitions=50                            \
    --benchmark_report_aggregates_only                    \
    --benchmark_out_format=json                           \
    --benchmark_out=before.json
```
for two different builds of the `execution-benchmarks` executable, producing two
output files `before.json` and `after.json`. Then, run (for example):
```console
$ uv run --directory scripts/benchmark-analysis   \
    compare-benchmarks                            \
    --before before.json                          \
    --after after.json                            \
    --type run                                    \
    --implementation interpreter                  \
    --aggregate
```
to generate a Markdown table comparing the results in the two JSON files.

To compare two different implementations on the same set of benchmarks, first
generate a single JSON output file with:
```console
$ ./build/test/execution_benchmarks/execution-benchmarks  \
    --benchmark_repetitions=50                            \
    --benchmark_report_aggregates_only                    \
    --benchmark_out_format=json                           \
    --benchmark_out=results.json
```
as above. Then, run:
```console
$ uv run --directory scripts/benchmark-analysis \
    compare-implementations results.json evmone interpreter
```
to generate a table showing the speedup of the `interpreter` implementation over
`evmone`.